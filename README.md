Natural language processing has recently drawn attention in the branch of Artificial Intelligence which helps computers make sense of human language. Studies in NLP has made it such that the gap in between human communication and computers is minimizing. Some of the applications of it include Speech recognition, customer support bot, and language identifier and so on. 

The main architecture of Neural networks involves around the interpretation of output and repeating the sequence of learning until the desired output is met. A neural network is composed of inputs or a set of connecting links; each link can be said to be a weight. These network inputs are then put into an activation function and given a necessary bias to optimize the output.

Recurrent Neural networks have the concept of the sequential memory, and they make use of the previous information which directly influences the later information. Long short-term memory are the upgraded versions of RNNs and helps solve the vanishing gradient problem by making use of gates in each cell of the network. These gates help differentiate which data in the long sequence is important to retain and which one to forget and hence uses the relevant information to help make the prediction. Thus, we see that LSTMs (Long Short-Term Memory) are useful for tackling the problem of text generation as we have to memorize large amount of previous data.
                                
The model has been implemented using Tensorflow and keras library in google colab. Simulation results have also been obtained for different combinations of activation functions and the training accuracy and loss have been shown respectively. Using these activation functions and different input sentences, different poems were generated.
